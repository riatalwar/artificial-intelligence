{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence\n",
    "# 464\n",
    "# Project #5\n",
    "\n",
    "## Before You Begin...\n",
    "00. We're using a Jupyter Notebook environment (tutorial available here: https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html),\n",
    "01. Read the entire notebook before beginning your work, and\n",
    "02.  Check the submission deadline on Gradescope.\n",
    "\n",
    "\n",
    "## General Directions for this Assignment\n",
    "00. Output format should be exactly as requested,\n",
    "01. Functions should do only one thing,\n",
    "02. Keep functions to 20 lines or less (empty lines are fine, there's leeway, but don't blatantly ignore this),\n",
    "03. Add docstring to all functions,\n",
    "\n",
    "\n",
    "## Before You Submit...\n",
    "00. Re-read the general instructions provided above, and\n",
    "01. Hit \"Kernel\"->\"Restart & Run All\". The first cell that is run should show [1], the second should show [2], and so on...\n",
    "02. Submit your notebook (as .ipynb, not PDF) using Gradescope, and\n",
    "03.  Do not submit any other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "For this assignment we will implement a Decision Tree using the ID3 Algorithm. The goal is classify a mushroom as either edible ('e') or poisonous ('p') using the same dataset as Project #5.\n",
    "\n",
    "\n",
    "Our  Decision Tree pipeline is as follows:\n",
    "\n",
    "\n",
    "1) `cross_validate` will take data (supplied as folds using 10 fold cross validation) and do the following:\n",
    "* For each setting of depth limit (the hyperparameter in decision trees, including 0)\n",
    "* * and for each fold of data\n",
    "* * * use `create_train_test` to split current fold into train and test\n",
    "* * * call `train` to build and return a decision tree, \n",
    "* * * call `classify` to use the tree to get classifications,\n",
    "* * * call `evaluate` to compare classifications to the actual answers (ground truth),\n",
    "* * * Print the performance for that fold\n",
    "* * Summarize the performance for that depth limit over all folds using `get_stats`\n",
    "\n",
    "\n",
    "2) `pretty_print_tree(tree)` will print what the tree looks like when using the **entire** data set (no train/test split) with depth limit set to None.\n",
    "\n",
    "\n",
    "All the code in this pipeline has been provided, except for a working `train` function. The `train` function currently returns a hard-coded tree from our lecture. Don't do that. Use ID3 to build your tree and use the depth limit to stop. When you're train function is complete, it should work for the lecture data, and mushrooms. Although `train` is terrible right now, pay attention to how the tree is structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Tuple, Callable\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"note\"></a>\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Note</strong>\n",
    "    <p>\n",
    "        Let's start with our example from the 08-April lecture. Target variable is Safe?, which can be yes or no. Anything *_lecture refers to the dataset we walked through in class.  \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lecture = [['round','large','blue','no'],\n",
    "['square','large','green','yes'],\n",
    "['square','small','red','no'],\n",
    "['round','large','red','yes'],\n",
    "['square','small','blue','no'],\n",
    "['round','small','blue','no'],\n",
    "['round','small','red','yes'],\n",
    "['square','small','green','no'],\n",
    "['round','large','green','yes'],\n",
    "['square','large','green','yes'],\n",
    "['square','large','red','no'],\n",
    "['square','large','green','yes'],\n",
    "['round','large','red','yes'],\n",
    "['square','small','red','no'],\n",
    "['round','small','green','no']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['round', 'large', 'blue', 'no']\n"
     ]
    }
   ],
   "source": [
    "print(data_lecture[0]) # a record of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_names_lecture = ['shape', \n",
    "                      'size', \n",
    "                      'color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(data: List, n: int) -> List[List[List]]:\n",
    "    \"\"\"\n",
    "    With n-fold cross validation, we divide our data set into n subgroups called \"folds\" and then use those folds for training and testing. \n",
    "    For data set with 100 observations (or records), n set to 10 would have 10 observations in each fold.\n",
    "    Arguments: \n",
    "        **data** List: a list (data_lecture, for instance)\n",
    "        **n** int: number of folds\n",
    "    Returns:\n",
    "        folds, which is a list of n items, where each item is a list containing a subgroup of xs\n",
    "    \"\"\"\n",
    "    k, m = divmod(len(data), n)\n",
    "    return list(data[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_lecture = create_folds(data=data_lecture, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds_lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['round', 'large', 'blue', 'no'], ['square', 'large', 'green', 'yes']]\n"
     ]
    }
   ],
   "source": [
    "print(folds_lecture[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['square', 'small', 'red', 'no'], ['round', 'large', 'red', 'yes']]\n"
     ]
    }
   ],
   "source": [
    "print(folds_lecture[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(folds: List[List[List]], index: int) -> Tuple[List[List], List[List]]:\n",
    "    \"\"\"\n",
    "    This function takes the n folds and returns the train and test sets. One of the n folds is used to test, the others are used for training.\n",
    "    Argumnents:\n",
    "        **folds** List[List[List]]: see `create_folds`\n",
    "        **index** int: fold index that is used for testing\n",
    "    Returns:\n",
    "        folds, which is a list of n items, where each item is a list containing a subgroup of xs\n",
    "    \"\"\"\n",
    "    training = []\n",
    "    test = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        if i == index:\n",
    "            test = fold\n",
    "        else:\n",
    "            training = training + fold\n",
    "    return training, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lecture, test_lecture = create_train_test(folds_lecture, 0) # test data is folds_lecture index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['square', 'small', 'red', 'no'], ['round', 'large', 'red', 'yes'], ['square', 'small', 'blue', 'no'], ['round', 'small', 'blue', 'no'], ['round', 'small', 'red', 'yes'], ['square', 'small', 'green', 'no'], ['round', 'large', 'green', 'yes'], ['square', 'large', 'green', 'yes'], ['square', 'large', 'red', 'no'], ['square', 'large', 'green', 'yes'], ['round', 'large', 'red', 'yes'], ['square', 'small', 'red', 'no'], ['round', 'small', 'green', 'no']]\n"
     ]
    }
   ],
   "source": [
    "print(train_lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['round', 'large', 'blue', 'no'], ['square', 'large', 'green', 'yes']]\n"
     ]
    }
   ],
   "source": [
    "print(test_lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lecture, test_lecture = create_train_test(folds_lecture, 1) # test data is folds_lecture index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['round', 'large', 'blue', 'no'], ['square', 'large', 'green', 'yes'], ['square', 'small', 'blue', 'no'], ['round', 'small', 'blue', 'no'], ['round', 'small', 'red', 'yes'], ['square', 'small', 'green', 'no'], ['round', 'large', 'green', 'yes'], ['square', 'large', 'green', 'yes'], ['square', 'large', 'red', 'no'], ['square', 'large', 'green', 'yes'], ['round', 'large', 'red', 'yes'], ['square', 'small', 'red', 'no'], ['round', 'small', 'green', 'no']]\n"
     ]
    }
   ],
   "source": [
    "print(train_lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['square', 'small', 'red', 'no'], ['round', 'large', 'red', 'yes']]\n"
     ]
    }
   ],
   "source": [
    "print(test_lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"note\"></a>\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <p>\n",
    "        Let's load the mushroom data.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file_name: str) -> List[List]:\n",
    "    \"\"\"\n",
    "    Opens a file, splits on comma, and shuffles data before returning as a List of list. \n",
    "    Arguments:\n",
    "        **file_name** Str: filename for data\n",
    "    Returns:\n",
    "        Data as a list of a list.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        datum = [value for value in line.rstrip().split(\",\")]\n",
    "        data.append(datum)\n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mushroom = parse_data(\"agaricus-lepiota.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Important</strong>\n",
    "    <p>\n",
    "        We're going to move the target column (mushroom edible or poisonous) to the last column to match the lecture's format, where Safe? was at the end.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mushroom = [record[1:]+[record[0]] for record in data_mushroom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8124"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_mushroom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['k', 'y', 'n', 'f', 's', 'f', 'c', 'n', 'b', 't', '?', 'k', 's', 'w', 'w', 'p', 'w', 'o', 'e', 'w', 'v', 'l', 'p']\n"
     ]
    }
   ],
   "source": [
    "print(data_mushroom[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_names_mushroom = ['cap-shape',\n",
    "                   'cap-surface',\n",
    "                   'cap-color',\n",
    "                   'bruises?',\n",
    "                   'odor',\n",
    "                   'gill-attachment',\n",
    "                   'gill-spacing',\n",
    "                   'gill-size',\n",
    "                   'gill-color',\n",
    "                   'stalk-shape',\n",
    "                   'stalk-root',\n",
    "                   'stalk-surface-above-ring',\n",
    "                   'stalk-surface-below-ring',\n",
    "                   'stalk-color-above-ring',\n",
    "                   'stalk-color-below-ring',\n",
    "                   'veil-type',\n",
    "                   'veil-color',\n",
    "                   'ring-number',\n",
    "                   'ring-type',\n",
    "                   'spore-print-color',\n",
    "                   'population',\n",
    "                   'habitat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(data):\n",
    "    \"\"\"\n",
    "    This function extracts a list of the target values from data. The function assumes the target variable is the last column of the data.\n",
    "    Arguments:\n",
    "        **data** List[List]: The data provided in a list of list format identical to the structure of `data_lecture` or `data_mushroom`\n",
    "    Returns:\n",
    "        A list of the values of the target variable.\n",
    "    \"\"\"\n",
    "    return [record[-1] for record in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_answers([]) == []\n",
    "assert get_answers(data_lecture) == ['no', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'yes', 'yes', 'no', 'no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mode(answers):\n",
    "    \"\"\"\n",
    "    This function finds the mode of a list of items. \n",
    "    Arguments:\n",
    "        **answers** List: A list of items\n",
    "    Returns:\n",
    "        The item that appears the most often in the list. \n",
    "    \"\"\"\n",
    "    count_dict = {}\n",
    "    for answer in answers:\n",
    "        if answer in count_dict:\n",
    "            count_dict[answer] = count_dict[answer] + 1\n",
    "        else:\n",
    "            count_dict[answer] = 1\n",
    "    mode_count = max(count_dict.values())\n",
    "    mode = [k for k, v in count_dict.items() if v == mode_count]\n",
    "    return mode[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_mode(['no', 'no', 'no', 'yes']) == 'no'\n",
    "assert get_mode(['no', 'no', 'yes', 'yes']) == 'no'\n",
    "assert get_mode(['no', 'yes', 'yes', 'yes']) == 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(data, attribute):\n",
    "  '''\n",
    "  Computes the entropy for a given attribute\n",
    "  \n",
    "  Params:\n",
    "    data (DataFrame): dataset\n",
    "    attribute (string): column in the dataset\n",
    "    \n",
    "  Returns:\n",
    "    float: entropy\n",
    "  '''\n",
    "  entropy = 0\n",
    "  vals = data[attribute].unique()\n",
    "  \n",
    "  for v in vals:\n",
    "    p = (data[attribute] == v).sum() / len(data)\n",
    "    if p > 0:\n",
    "      entropy -= p * math.log2(p)\n",
    "\n",
    "  return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_information_gain(data, attribute, entropy):\n",
    "  '''\n",
    "  Computes the information gain for a given attribute\n",
    "  \n",
    "  Params:\n",
    "    data (DataFrame): dataset\n",
    "    attribute (string): column in the dataset\n",
    "    entropy (float): base entropy of the data\n",
    "    \n",
    "  Returns:\n",
    "    float: information gain\n",
    "  '''\n",
    "  vals = data[attribute].unique()\n",
    "\n",
    "  # sum entropy of dataset for each subset (split by attribute value)\n",
    "  weightedEntropy = 0\n",
    "  for v in vals:\n",
    "    subset = data[data[attribute] == v]\n",
    "    weightedEntropy += compute_entropy(subset, 'result') * (len(subset) / len(data))\n",
    "\n",
    "  return entropy - weightedEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_information_gain(data, used_attributes):\n",
    "  '''\n",
    "  Find the attribute with the best information gain\n",
    "  \n",
    "  Params:\n",
    "    data (DataFrame): dataset\n",
    "    used_attributes (Set[string]): attributes that have already been used for decisions in the tree\n",
    "    \n",
    "  Returns:\n",
    "    string: best attribute\n",
    "  '''\n",
    "  entropy = compute_entropy(data, 'result')\n",
    "\n",
    "  bestAttr = None\n",
    "  bestGain = -sys.maxsize - 1\n",
    "  # compute information gain for each attribute\n",
    "  for attr in data.columns:\n",
    "    if attr in used_attributes: continue\n",
    "    infoGain = compute_information_gain(data, attr, entropy)\n",
    "    if infoGain > bestGain:\n",
    "      bestGain = infoGain\n",
    "      bestAttr = attr\n",
    "\n",
    "  return bestAttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tree(data_df, used_attributes, depth):\n",
    "  '''\n",
    "  Computes the entropy for a given attribute\n",
    "  \n",
    "  Params:\n",
    "    data (DataFrame): dataset\n",
    "    used_attributes (Set[string]): attributes that have already been used for decisions in the tree\n",
    "    depth (int): max levels in the tree\n",
    "    \n",
    "  Returns:\n",
    "    Dict: The trained decision tree using the ID3 algorithm (entropy, information gain). \n",
    "      It is represented as a nested dictionary. \n",
    "  '''\n",
    "  # find attribute with best info gain\n",
    "  bestGainAttr = find_best_information_gain(data_df, used_attributes)\n",
    "  if depth == 0 or bestGainAttr == None:\n",
    "    return data_df['result'].mode()[0]\n",
    "  \n",
    "  used_attributes.add(bestGainAttr)\n",
    "  vals = data_df[bestGainAttr].unique()\n",
    "\n",
    "  # compute subtrees or endpoints for each category in attribute\n",
    "  tree = dict()\n",
    "  for v in vals:\n",
    "    subset = data_df[data_df[bestGainAttr] == v]\n",
    "    key = (bestGainAttr, data_df.columns.get_loc(bestGainAttr), v)\n",
    "    \n",
    "    # Check if can cut off branch here if result is homogenous\n",
    "    mode = subset['result'].mode()[0]\n",
    "    resultCount = subset['result'].value_counts()[mode]\n",
    "    if resultCount != len(subset):\n",
    "      subtree = compute_tree(subset, deepcopy(used_attributes), depth - 1 if depth is not None else depth)\n",
    "      tree[key] = subtree\n",
    "    else:\n",
    "      tree[key] = mode\n",
    "\n",
    "  return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data, attribute_names, depth_limit=None):\n",
    "    \"\"\"\n",
    "    This function takes training_data, attribute names, and the depth limit and returns the decision tree as a nested dictionary. \n",
    "    If the depth is 0, a dictionary is not returned. Instead, the mode of the target values is returned (i.e., majority class). \n",
    "    Arguments:\n",
    "        **training_data** List[List]: The data\n",
    "        **attribute_names** List: The attribute names of the data (22 for mushroom; size, shape, and color for the lecture)\n",
    "        **depth_limit** int: The depth limit of the tree\n",
    "    Returns:\n",
    "        **dt** Dict: The trained decision tree using the ID3 algorithm (entropy, information gain). \n",
    "        It is represented as a nested dictionary. \n",
    "        The dictionary returned for the lecture is structured as below:\n",
    "        {\n",
    "        ('size', 1, 'large'): \n",
    "            {('color', 2, 'blue'): 'no', \n",
    "            ('color', 2, 'green'): 'yes', \n",
    "            ('color', 2, 'red'): \n",
    "                {('shape', 0, 'round'): 'yes', \n",
    "                ('shape', 0, 'square'): 'no'}\n",
    "            }, \n",
    "        ('size', 1, 'small'): \n",
    "            {('shape', 0, 'square'): 'no', \n",
    "            ('shape', 0, 'round'): \n",
    "                {('color', 2, 'blue'): 'no', \n",
    "                ('color', 2, 'red'): 'yes', \n",
    "                ('color', 2, 'green'): 'no'}\n",
    "            }\n",
    "        }\n",
    "        Notice that the keys are tuples; for instance, ('size', 1, 'large') is a key. \n",
    "        The key includes the attribute's name, column number in data, and value.\n",
    "        The function currently returns a hard-coded tree. \n",
    "        Your implementation should replace this with a tree that is learned from the data using the ID3 algorithm. \n",
    "        You do not have to assert test `train`, but it may be worthwhile to check that it can return the tree from the lecture once your implementation is in place.\n",
    "    \"\"\"\n",
    "    data_df = pd.DataFrame(training_data, columns=[*attribute_names, 'result'])\n",
    "    return compute_tree(data_df, set(['result']), depth_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "dt_lecture = train(training_data=train_lecture, attribute_names=attribute_names_lecture, depth_limit=0)\n",
    "print(dt_lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(record, dt):\n",
    "    \"\"\"\n",
    "    This recursive function uses a decision tree represented as a nested dictionary get a prediction from a record, which is a row of the data. \n",
    "    Arguments:\n",
    "        **record** List[]: A row of data to be predicted\n",
    "        **dt** the decision tree used to make the prediction\n",
    "    Returns:\n",
    "        A prediction ('yes' or 'no' for instance, from our Self Check example.) \n",
    "    \"\"\"\n",
    "    if not isinstance(dt, dict):\n",
    "        return dt\n",
    "    else:\n",
    "        for key, value in dt.items():\n",
    "            if record[key[1]]==key[2]:\n",
    "                return get_prediction(record, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "no\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "print(get_prediction(['round','large','blue','no'], dt=dt_lecture))\n",
    "print(get_prediction(['square','large','green','yes'], dt=dt_lecture))\n",
    "print(get_prediction(['square','small','red','no'], dt=dt_lecture))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(dt, observations):\n",
    "    \"\"\"\n",
    "    This function takes a decision tree, observations, and a labeled flag to return a list of classifications. \n",
    "    Arguments:\n",
    "        **dt** Dict: The decision tree as a nested dictionary\n",
    "        **observation** List[List]: a list of items, where each item is a row of the data\n",
    "        **labeled** Bool: true for labeled data\n",
    "    Returns:\n",
    "        **y_hat** List: A list of classifications.\n",
    "    \"\"\"\n",
    "    y_hat = []\n",
    "    for record in observations:\n",
    "        y_hat.append(get_prediction(record, dt))   \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no', 'no']\n"
     ]
    }
   ],
   "source": [
    "print(classify(dt=dt_lecture, observations=test_lecture))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: \"\\_\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\_\"? A raw string is also an option.\n",
      "<>:5: SyntaxWarning: \"\\_\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\_\"? A raw string is also an option.\n",
      "/var/folders/sf/x6gc4f816zddh9n8472r40qh0000gn/T/ipykernel_55543/3381848552.py:5: SyntaxWarning: \"\\_\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\_\"? A raw string is also an option.\n",
      "  $$error\\_rate=\\frac{errors}{n}$$\n"
     ]
    }
   ],
   "source": [
    "def evaluate(y_hat, observations):\n",
    "    \"\"\"\n",
    "    This function evaluates the performance of a classifier. \n",
    "    It takes a data set (training set or test set) and the classification result (see [classify](#classify) above and calculates the classification error rate:\n",
    "        $$error\\_rate=\\frac{errors}{n}$$ \n",
    "    Arguments:\n",
    "        **y_hat** List: A list of predictions\n",
    "        **observations** List[List]: Data to be predicted (typically training or test set)\n",
    "    Returns:\n",
    "        **error_rate** float: The error rate.\n",
    "    \"\"\"\n",
    "    errors = 0\n",
    "    ground_truth = get_answers(observations)\n",
    "    for index in range(len(y_hat)):\n",
    "        if y_hat[index] != ground_truth[index]:\n",
    "            errors = errors + 1\n",
    "    return errors / (len(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(classify(dt=dt_lecture, observations=data_lecture), observations=data_lecture))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_stats\"></a>\n",
    "## get_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(observations: List[float]) -> Tuple[float,float]:\n",
    "    \"\"\"\n",
    "    This function computes the mean and the standard deviation for a given list of observations. \n",
    "    Arguments:\n",
    "        **observations** List[float]: A list of observations\n",
    "    Returns:\n",
    "        (mean, standard deviation) Tuple[float,float]: tuple consisting of mean and the standard deviation\n",
    "    \"\"\"\n",
    "    mean = sum(observations) / len(observations)\n",
    "    variance = sum([(elem - mean)**2 for elem in observations]) / len(observations)\n",
    "    std_dev = math.sqrt(variance)\n",
    "    return mean, std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_stats([2, 4, 4, 4, 5, 5, 7, 9]) == (5.0, 2.0)\n",
    "assert get_stats([1, 1, 1]) == (1.0, 0.0)\n",
    "assert get_stats([0]) == (0.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(folds, attribute_names, hyperparameters):\n",
    "    \"\"\"\n",
    "    This function takes folds of data to `train`, `classify`, and `evaluate`.\n",
    "    Arguments:\n",
    "        **folds** List[List[List]]: The original dataset partitioned into folds (see `create_folds` above)\n",
    "        **attribute_names** int: the feature names\n",
    "        **hyperparameters** List: A list of hyperparameters to explore (depth limits for a decision tree, for instance)\n",
    "    Returns:\n",
    "        Nothing is returned, but for each hyperparameter setting, the function prints out the fold number and the error rate for that fold. \n",
    "        The mean and variance is printed across folds for each hyperparameter setting. \n",
    "        The error rates are reported in terms of percents.\n",
    "    \"\"\"\n",
    "    for hyperparameter in hyperparameters:\n",
    "        train_error, test_error  = [], []\n",
    "        error_list_train, error_list_test = [], []\n",
    "        for fold_index in range(len(folds)):\n",
    "            training_data, test_data = create_train_test(folds, fold_index)\n",
    "            tree = train(training_data=training_data, attribute_names=attribute_names, depth_limit=hyperparameter)\n",
    "            y_hat_train = classify(tree, training_data)\n",
    "            y_hat_test = classify(tree, test_data)\n",
    "            error_rate_train = evaluate(y_hat_train, training_data)\n",
    "            error_rate_test = evaluate(y_hat_test, test_data)\n",
    "            error_list_train.append(error_rate_train)\n",
    "            error_list_test.append(error_rate_test)\n",
    "            print(f\"Fold: {fold_index}\\tTrain Error: {error_rate_train*100:.2f}%\\tTest Error: {error_rate_test*100:.2f}%\")\n",
    "        print(f\"***\")\n",
    "        print(f\"Depth limit: {hyperparameter}\")\n",
    "        print(f\"\\nMean(Std. Dev.) over all folds:\\n-------------------------------\")\n",
    "        print(f\"Train Error: {get_stats(error_list_train)[0]*100:.2f}%({get_stats(error_list_train)[1]*100:.2f}%) Test Error: {get_stats(error_list_test)[0]*100:.2f}%({get_stats(error_list_test)[1]*100:.2f}%)\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\tTrain Error: 46.15%\tTest Error: 50.00%\n",
      "Fold: 1\tTrain Error: 46.15%\tTest Error: 50.00%\n",
      "Fold: 2\tTrain Error: 46.15%\tTest Error: 100.00%\n",
      "Fold: 3\tTrain Error: 46.15%\tTest Error: 50.00%\n",
      "Fold: 4\tTrain Error: 38.46%\tTest Error: 100.00%\n",
      "Fold: 5\tTrain Error: 50.00%\tTest Error: 0.00%\n",
      "Fold: 6\tTrain Error: 42.86%\tTest Error: 100.00%\n",
      "Fold: 7\tTrain Error: 42.86%\tTest Error: 100.00%\n",
      "Fold: 8\tTrain Error: 50.00%\tTest Error: 0.00%\n",
      "Fold: 9\tTrain Error: 50.00%\tTest Error: 0.00%\n",
      "***\n",
      "Depth limit: 0\n",
      "\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 45.88%(3.53%) Test Error: 55.00%(41.53%)\n",
      "\n",
      "\n",
      "Fold: 0\tTrain Error: 15.38%\tTest Error: 50.00%\n",
      "Fold: 1\tTrain Error: 30.77%\tTest Error: 50.00%\n",
      "Fold: 2\tTrain Error: 23.08%\tTest Error: 0.00%\n",
      "Fold: 3\tTrain Error: 15.38%\tTest Error: 50.00%\n",
      "Fold: 4\tTrain Error: 23.08%\tTest Error: 0.00%\n",
      "Fold: 5\tTrain Error: 14.29%\tTest Error: 100.00%\n",
      "Fold: 6\tTrain Error: 21.43%\tTest Error: 0.00%\n",
      "Fold: 7\tTrain Error: 21.43%\tTest Error: 0.00%\n",
      "Fold: 8\tTrain Error: 28.57%\tTest Error: 100.00%\n",
      "Fold: 9\tTrain Error: 28.57%\tTest Error: 100.00%\n",
      "***\n",
      "Depth limit: 1\n",
      "\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 22.20%(5.59%) Test Error: 45.00%(41.53%)\n",
      "\n",
      "\n",
      "Fold: 0\tTrain Error: 15.38%\tTest Error: 50.00%\n",
      "Fold: 1\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 2\tTrain Error: 15.38%\tTest Error: 0.00%\n",
      "Fold: 3\tTrain Error: 7.69%\tTest Error: 50.00%\n",
      "Fold: 4\tTrain Error: 15.38%\tTest Error: 0.00%\n",
      "Fold: 5\tTrain Error: 7.14%\tTest Error: 100.00%\n",
      "Fold: 6\tTrain Error: 14.29%\tTest Error: 0.00%\n",
      "Fold: 7\tTrain Error: 14.29%\tTest Error: 100.00%\n",
      "Fold: 8\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 9\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "***\n",
      "Depth limit: 2\n",
      "\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 8.96%(6.53%) Test Error: 30.00%(40.00%)\n",
      "\n",
      "\n",
      "Fold: 0\tTrain Error: 0.00%\tTest Error: 50.00%\n",
      "Fold: 1\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 2\tTrain Error: 0.00%\tTest Error: 50.00%\n",
      "Fold: 3\tTrain Error: 0.00%\tTest Error: 50.00%\n",
      "Fold: 4\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 5\tTrain Error: 0.00%\tTest Error: 100.00%\n",
      "Fold: 6\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 7\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 8\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 9\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "***\n",
      "Depth limit: 3\n",
      "\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 0.00%(0.00%) Test Error: 25.00%(33.54%)\n",
      "\n",
      "\n",
      "Fold: 0\tTrain Error: 0.00%\tTest Error: 50.00%\n",
      "Fold: 1\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 2\tTrain Error: 0.00%\tTest Error: 50.00%\n",
      "Fold: 3\tTrain Error: 0.00%\tTest Error: 50.00%\n",
      "Fold: 4\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 5\tTrain Error: 0.00%\tTest Error: 100.00%\n",
      "Fold: 6\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 7\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 8\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 9\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "***\n",
      "Depth limit: 4\n",
      "\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 0.00%(0.00%) Test Error: 25.00%(33.54%)\n",
      "\n",
      "\n",
      "Fold: 0\tTrain Error: 0.00%\tTest Error: 50.00%\n",
      "Fold: 1\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 2\tTrain Error: 0.00%\tTest Error: 50.00%\n",
      "Fold: 3\tTrain Error: 0.00%\tTest Error: 50.00%\n",
      "Fold: 4\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 5\tTrain Error: 0.00%\tTest Error: 100.00%\n",
      "Fold: 6\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 7\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 8\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 9\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "***\n",
      "Depth limit: 5\n",
      "\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 0.00%(0.00%) Test Error: 25.00%(33.54%)\n",
      "\n",
      "\n",
      "Fold: 0\tTrain Error: 0.00%\tTest Error: 50.00%\n",
      "Fold: 1\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 2\tTrain Error: 0.00%\tTest Error: 50.00%\n",
      "Fold: 3\tTrain Error: 0.00%\tTest Error: 50.00%\n",
      "Fold: 4\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 5\tTrain Error: 0.00%\tTest Error: 100.00%\n",
      "Fold: 6\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 7\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 8\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 9\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "***\n",
      "Depth limit: None\n",
      "\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 0.00%(0.00%) Test Error: 25.00%(33.54%)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_validate(folds=folds_lecture, attribute_names=attribute_names_lecture, hyperparameters=[0, 1, 2, 3, 4, 5, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_tree(dt, tab_space):\n",
    "    \"\"\"\n",
    "    This function provides a text-based representation of a decision tree that is represented as a nested dictionary. \n",
    "    Arguments:\n",
    "        **dt** Dict: The decision tree as a nested dictionary\n",
    "        **tab_space** Int: How much to tab successive depth levels of the resulting tree\n",
    "    \"\"\"\n",
    "    for key, value in dt.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(\"  \" * tab_space + str(key[0]).upper() + \" - \" + str(key[2]) + \": \")\n",
    "            print(\"\\n\")\n",
    "            pretty_print_tree(value, tab_space+3)\n",
    "        else:\n",
    "            print(\"  \" * tab_space + str(key[0]).upper() + \" - \" + str(key[2]) + \" =====> \" + str(value))\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE - large: \n",
      "\n",
      "\n",
      "      COLOR - blue =====> no\n",
      "\n",
      "\n",
      "      COLOR - green =====> yes\n",
      "\n",
      "\n",
      "      COLOR - red: \n",
      "\n",
      "\n",
      "            SHAPE - round =====> yes\n",
      "\n",
      "\n",
      "            SHAPE - square =====> no\n",
      "\n",
      "\n",
      "SIZE - small: \n",
      "\n",
      "\n",
      "      SHAPE - square =====> no\n",
      "\n",
      "\n",
      "      SHAPE - round: \n",
      "\n",
      "\n",
      "            COLOR - blue =====> no\n",
      "\n",
      "\n",
      "            COLOR - red =====> yes\n",
      "\n",
      "\n",
      "            COLOR - green =====> no\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_lecture = train(training_data=data_lecture, attribute_names=attribute_names_lecture, depth_limit=None)\n",
    "pretty_print_tree(dt_lecture, tab_space=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <p>\n",
    "        Let's work on the mushroom data. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify the Mushrooom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_mushroom = create_folds(data=data_mushroom, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\tTrain Error: 48.27%\tTest Error: 47.60%\n",
      "Fold: 1\tTrain Error: 47.86%\tTest Error: 51.29%\n",
      "Fold: 2\tTrain Error: 48.31%\tTest Error: 47.23%\n",
      "Fold: 3\tTrain Error: 48.31%\tTest Error: 47.23%\n",
      "Fold: 4\tTrain Error: 48.10%\tTest Error: 49.14%\n",
      "Fold: 5\tTrain Error: 48.07%\tTest Error: 49.38%\n",
      "Fold: 6\tTrain Error: 48.33%\tTest Error: 47.04%\n",
      "Fold: 7\tTrain Error: 47.99%\tTest Error: 50.12%\n",
      "Fold: 8\tTrain Error: 48.45%\tTest Error: 45.94%\n",
      "Fold: 9\tTrain Error: 48.33%\tTest Error: 47.04%\n",
      "***\n",
      "Depth limit: 0\n",
      "\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 48.20%(0.18%) Test Error: 48.20%(1.60%)\n",
      "\n",
      "\n",
      "Fold: 0\tTrain Error: 1.49%\tTest Error: 1.35%\n",
      "Fold: 1\tTrain Error: 1.45%\tTest Error: 1.72%\n",
      "Fold: 2\tTrain Error: 1.52%\tTest Error: 1.11%\n",
      "Fold: 3\tTrain Error: 1.45%\tTest Error: 1.72%\n",
      "Fold: 4\tTrain Error: 1.45%\tTest Error: 1.72%\n",
      "Fold: 5\tTrain Error: 1.49%\tTest Error: 1.35%\n",
      "Fold: 6\tTrain Error: 1.50%\tTest Error: 1.23%\n",
      "Fold: 7\tTrain Error: 1.46%\tTest Error: 1.60%\n",
      "Fold: 8\tTrain Error: 1.48%\tTest Error: 1.48%\n",
      "Fold: 9\tTrain Error: 1.48%\tTest Error: 1.48%\n",
      "***\n",
      "Depth limit: 1\n",
      "\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 1.48%(0.02%) Test Error: 1.48%(0.21%)\n",
      "\n",
      "\n",
      "Fold: 0\tTrain Error: 0.63%\tTest Error: 0.25%\n",
      "Fold: 1\tTrain Error: 0.57%\tTest Error: 0.74%\n",
      "Fold: 2\tTrain Error: 0.62%\tTest Error: 0.37%\n",
      "Fold: 3\tTrain Error: 0.59%\tTest Error: 0.62%\n",
      "Fold: 4\tTrain Error: 0.57%\tTest Error: 0.74%\n",
      "Fold: 5\tTrain Error: 0.59%\tTest Error: 0.62%\n",
      "Fold: 6\tTrain Error: 0.57%\tTest Error: 0.74%\n",
      "Fold: 7\tTrain Error: 0.59%\tTest Error: 0.62%\n",
      "Fold: 8\tTrain Error: 0.62%\tTest Error: 0.37%\n",
      "Fold: 9\tTrain Error: 0.56%\tTest Error: 0.86%\n",
      "***\n",
      "Depth limit: 2\n",
      "\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 0.59%(0.02%) Test Error: 0.59%(0.19%)\n",
      "\n",
      "\n",
      "Fold: 0\tTrain Error: 0.33%\tTest Error: 0.00%\n",
      "Fold: 1\tTrain Error: 0.27%\tTest Error: 0.49%\n",
      "Fold: 2\tTrain Error: 0.29%\tTest Error: 0.37%\n",
      "Fold: 3\tTrain Error: 0.27%\tTest Error: 0.49%\n",
      "Fold: 4\tTrain Error: 0.31%\tTest Error: 0.12%\n",
      "Fold: 5\tTrain Error: 0.27%\tTest Error: 0.49%\n",
      "Fold: 6\tTrain Error: 0.31%\tTest Error: 0.12%\n",
      "Fold: 7\tTrain Error: 0.27%\tTest Error: 0.49%\n",
      "Fold: 8\tTrain Error: 0.31%\tTest Error: 0.12%\n",
      "Fold: 9\tTrain Error: 0.30%\tTest Error: 0.25%\n",
      "***\n",
      "Depth limit: 3\n",
      "\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 0.30%(0.02%) Test Error: 0.30%(0.18%)\n",
      "\n",
      "\n",
      "Fold: 0\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 1\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 2\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 3\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 4\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 5\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 6\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 7\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 8\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 9\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "***\n",
      "Depth limit: 4\n",
      "\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 0.00%(0.00%) Test Error: 0.00%(0.00%)\n",
      "\n",
      "\n",
      "Fold: 0\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 1\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 2\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 3\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 4\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 5\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 6\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 7\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 8\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 9\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "***\n",
      "Depth limit: 5\n",
      "\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 0.00%(0.00%) Test Error: 0.00%(0.00%)\n",
      "\n",
      "\n",
      "Fold: 0\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 1\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 2\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 3\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 4\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 5\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 6\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 7\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 8\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 9\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "***\n",
      "Depth limit: None\n",
      "\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 0.00%(0.00%) Test Error: 0.00%(0.00%)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_validate(folds=folds_mushroom, attribute_names=attribute_names_mushroom, hyperparameters=[0, 1, 2, 3, 4, 5, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <p>\n",
    "        Let's work on the mushroom data. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the Mushroom Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ODOR - s =====> p\n",
      "\n",
      "\n",
      "ODOR - l =====> e\n",
      "\n",
      "\n",
      "ODOR - a =====> e\n",
      "\n",
      "\n",
      "ODOR - f =====> p\n",
      "\n",
      "\n",
      "ODOR - n: \n",
      "\n",
      "\n",
      "      SPORE-PRINT-COLOR - n =====> e\n",
      "\n",
      "\n",
      "      SPORE-PRINT-COLOR - k =====> e\n",
      "\n",
      "\n",
      "      SPORE-PRINT-COLOR - w: \n",
      "\n",
      "\n",
      "            HABITAT - g =====> e\n",
      "\n",
      "\n",
      "            HABITAT - w =====> e\n",
      "\n",
      "\n",
      "            HABITAT - l: \n",
      "\n",
      "\n",
      "                  CAP-COLOR - n =====> e\n",
      "\n",
      "\n",
      "                  CAP-COLOR - c =====> e\n",
      "\n",
      "\n",
      "                  CAP-COLOR - y =====> p\n",
      "\n",
      "\n",
      "                  CAP-COLOR - w =====> p\n",
      "\n",
      "\n",
      "            HABITAT - d: \n",
      "\n",
      "\n",
      "                  GILL-SIZE - n =====> p\n",
      "\n",
      "\n",
      "                  GILL-SIZE - b =====> e\n",
      "\n",
      "\n",
      "            HABITAT - p =====> e\n",
      "\n",
      "\n",
      "      SPORE-PRINT-COLOR - r =====> p\n",
      "\n",
      "\n",
      "      SPORE-PRINT-COLOR - y =====> e\n",
      "\n",
      "\n",
      "      SPORE-PRINT-COLOR - o =====> e\n",
      "\n",
      "\n",
      "      SPORE-PRINT-COLOR - b =====> e\n",
      "\n",
      "\n",
      "      SPORE-PRINT-COLOR - h =====> e\n",
      "\n",
      "\n",
      "ODOR - c =====> p\n",
      "\n",
      "\n",
      "ODOR - y =====> p\n",
      "\n",
      "\n",
      "ODOR - p =====> p\n",
      "\n",
      "\n",
      "ODOR - m =====> p\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_mushroom = train(training_data=data_mushroom, attribute_names=attribute_names_mushroom, depth_limit=None)\n",
    "pretty_print_tree(dt_mushroom, tab_space=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a library for the following:\n",
    "\n",
    "Use an AI/ML library to build and display the best tree using your chosen library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation goes here.\n",
    "# TODO: apply ai/ml library\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "00. Re-read the general instructions provided above, and\n",
    "01. Hit \"Kernel\"->\"Restart & Run All\". The first cell that is run should show [1], the second should show [2], and so on...\n",
    "02. Submit your notebook (as .ipynb, not PDF) using Gradescope, and\n",
    "03.  Do not submit any other files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
