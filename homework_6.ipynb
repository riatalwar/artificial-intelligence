{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84GqNz8Ztw4G"
   },
   "source": [
    "# Artificial Intelligence\n",
    "# 464\n",
    "# Homework #6\n",
    "\n",
    "## Before You Begin...\n",
    "00. We're using a Jupyter Notebook environment (tutorial available here: https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html),\n",
    "01. Read the entire notebook before beginning your work, and\n",
    "02.  Check the submission deadline on Gradescope.\n",
    "\n",
    "\n",
    "## General Directions for this Assignment\n",
    "00. Output format should be exactly as requested (it is your responsibility to make sure notebook looks as expected on Gradescope), and\n",
    "01. Functions should do only one thing.\n",
    "\n",
    "\n",
    "## Before You Submit...\n",
    "00. Re-read the general instructions provided above, and\n",
    "01. Hit \"Kernel\"->\"Restart & Run All\". The first cell that is run should show [1], the second should show [2], and so on...\n",
    "02. Submit your notebook (as .ipynb, not PDF) using Gradescope, and\n",
    "03.  Do not submit any other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSN1KpKJtw4K"
   },
   "source": [
    "## Language Modeling\n",
    "\n",
    "This homework will require you to load and train models.  If you choose small models and datasets, you should be able to run this locally on your computer. However, larger models/datasets may require GPU access. You can access one GPU for free on [Google Colab](https://colab.research.google.com/).\n",
    "\n",
    "We will use HuggingFace libraries in this quiz. We discussed majority of what you will need during the discussion demo. Additional documentation can be found [here](https://huggingface.co/docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3-RPRoiXtw4L"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riata/Documents/src/artificial-intelligence/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeaSsuWCt2dI"
   },
   "source": [
    "## Problem 0: Data\n",
    "From the [HuggingFace Datasets](https://huggingface.co/datasets), choose a dataset that satisfies the following criteria:\n",
    "- Data must have train and test splits (Optional development set)\n",
    "- Task must be text classification\n",
    "- Task must have at least 3 labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WaYqkeqQyanB"
   },
   "outputs": [],
   "source": [
    "ds_name = \"dair-ai/emotion\"\n",
    "# Load the data here\n",
    "ds = load_dataset(ds_name, \"split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H64O9-k8f4BT"
   },
   "source": [
    "**Describe the data.**\n",
    "What is the utility of the task? What are the inputs? What are the labels? Are the any potential difficulties you expect from the task? How do you evaluate the performance of this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsVba2Amf4BU"
   },
   "source": [
    "In this dataset, the inputs are text from Twitter messages that are labeled with one of \"six basic emotions: anger, fear, joy, love, sadness, and surprise.\" I think that it would be easy for confusion to arise due to the subjective nature of emotion and the lack of ability to determine intent behind a phrase. Tools like sarcasm can give a very different meaning to a phrase than its literal interpretation, likely causing some difficulty in classification. To evaluate the performance, we would need to determine the accuracy of classification, including the frequency of various misclassifications (e.g. how often anger is interpreted as sadness)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huBfVtgPf4BU"
   },
   "source": [
    "**Research current methods using this dataset.**\n",
    "What is the current state of the art method? Describe the method, including the type of model used, training protocol (if any), and the performance. Cite your sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7bwYwd_f4BV"
   },
   "source": [
    "The current state-of the art is the BERT transformer model, which seems to outperform most other models with its enhanced ability to detect subtle emotional cues (Shah et al).  Many researchers also look at this technique to see how it can be integrated into other methods such as CNNs to enhance the effectiveness of both (Abas et al; Bhardwaj and Abulaish). Training these models requires large, labeled text datasets that classify strings of text with various standard emotions, something that can be difficult to find. This model is made up of transformer encoder layers with several nodes, and for every input in a series, each node calculates part to form the vector representation. The outputs of each node are combined, and the layer is then normalized (Abas et al)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citations:\n",
    "Bhardwaj and Abulaish: https://www.sciencedirect.com/science/article/pii/S2666827025000763\n",
    "Khemani et al: https://pmc.ncbi.nlm.nih.gov/articles/PMC12148580/\n",
    "Shah et al: https://www.science-gate.com/IJAAS/2025/V12I7/1021833ijaas202507006.html\n",
    "Abas et el: https://www.sciencedirect.com/org/science/article/pii/S1546221821001314"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hTGXzrBf4BV"
   },
   "source": [
    "(Optional) If necessary, perform any data preprocessing here. For example, depending on the dataset you choose, you may need to clean the text or split the training set into a train and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_9F3l635f4BV"
   },
   "outputs": [],
   "source": [
    "# TODO: Dataset preprocessing (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ub3eini5f4BV"
   },
   "source": [
    "## Problem 1: encoder-only models or decoder-only models\n",
    "## Option A: encoder-only models\n",
    "Choose an encoder-only model (e.g. BERT). Load the model and add a classification layer.\n",
    "\n",
    "Describe the model you choose. What are the unique properties of this model? What are the pros and cons? Cite your sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A47fH4kQf4BW"
   },
   "source": [
    "The bert-base-uncased model is a transformer model trained on a large quantitiy of english text. The uncased attribute means that it does not distinguish between different cases. One of its primary features is masked language modeling (MLM), where 15% of the input text is randomly masked so that the model can predict the missing text. This helps the model learn to understand and leverage the context surrounding a word. The second feature is next sentence prediction (NSP), which concatenates two of the masked sentences. The model is then made to predict whether or not the two sentences actually followed each other or not. The model is ideal for fine-tuning, MLM, and NSP, but for full text generation there are beter choices.\n",
    "\n",
    "Source: https://huggingface.co/google-bert/bert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwklqMwuf4BW"
   },
   "source": [
    "Finetune the model on your dataset. Report the performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# source: https://huggingface.co/docs/transformers/en/training\n",
    "# select model\n",
    "model_name = \"distilbert/distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize data\n",
    "def tokenize_dataset(dataset):\n",
    "    return tokenizer(dataset[\"text\"])\n",
    "tokenized_ds = ds.map(tokenize_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model evaluation\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"accuracy\", \"f1\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training args\n",
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"spam-detect\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riata/Documents/src/artificial-intelligence/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:36, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.502300</td>\n",
       "      <td>1.146636</td>\n",
       "      <td>0.609000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.098200</td>\n",
       "      <td>1.007736</td>\n",
       "      <td>0.666000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riata/Documents/src/artificial-intelligence/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=1.3002310791015625, metrics={'train_runtime': 36.7976, 'train_samples_per_second': 54.351, 'train_steps_per_second': 6.794, 'total_flos': 21840050384352.0, 'train_loss': 1.3002310791015625, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take small chunk of dataset for training and testing\n",
    "train = tokenized_ds[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "test = tokenized_ds[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# set up model trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riata/Documents/src/artificial-intelligence/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get model evaluation metrics\n",
    "predictions = trainer.predict(tokenized_ds[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "\n",
    "def map_label(label):\n",
    "  return mapping[label]\n",
    "\n",
    "def parse(response):\n",
    "  response = response.lower()\n",
    "  for emot in mapping:\n",
    "    if emot in response:\n",
    "      return emot\n",
    "    \n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 1.0157246589660645, 'test_accuracy': 0.664, 'test_runtime': 5.8649, 'test_samples_per_second': 341.013, 'test_steps_per_second': 42.627}\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "print(predictions.metrics)\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Print a few example predictions\n",
    "# for i in range(10):\n",
    "#     print(f\"text: {tokenized_ds['test'][i]['text']}, response: {map_label(predicted_labels[i])}, actual: {map_label(true_labels[i])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyRCZIpuf4BW"
   },
   "source": [
    "## Option B: decoder-only models\n",
    "Choose an decoder-only model (e.g. GPT2). Describe the model you choose. What are the unique properties of this model? What are the pros and cons? Cite your sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WEEC_Dpf4BW"
   },
   "source": [
    "The SmolLM2 model is a language model trained on large datasets with supervised fine-tuning. Though the model is not as targeted for classification tasks as the previous model we looked at, its ability to take in any context and base its responses accordingly allow it to be used in a much wider range of scenarios. This does reduce the assuracy, but as we saw in class, we were able to make it act like an \"evil devil\" and \"spam detector,\" which are very different but can both be achieved through prompting.\n",
    "\n",
    "Source: https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VebSjZIof4BX"
   },
   "source": [
    "Load the model and use prompting for your task. You will likely need to write a helper function to parse the answer.\n",
    "\n",
    "(Ex. “The answer is 1” -> 1). Report the performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "device = Accelerator().device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "chatbot = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"This is a text classification task.\n",
    "text: {text}\n",
    "Is this text anger, fear, joy, love, sadness, or surprise:\"\"\"\n",
    "\n",
    "def create_chat(text):\n",
    "  return [\n",
    "    {\"role\": \"system\", \"content\": \"\"\"You are an emotion classifier agent that looks at a string of text \n",
    "      and outputs the emotion it expresses. The emotion must be one of the following \n",
    "      6 options: anger, fear, joy, love, sadness, and surprise.\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{prompt.format(text=text)}\"},\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success rate: 0.5\n",
      "error rate: 0.5\n"
     ]
    }
   ],
   "source": [
    "success = 0\n",
    "failure = 0\n",
    "total = 100\n",
    "# test a series of example text\n",
    "for i in range(total):\n",
    "  text = ds[\"test\"][i][\"text\"]\n",
    "  actual = ds[\"test\"][i][\"label\"]\n",
    "\n",
    "  # generate response\n",
    "  chat = create_chat(text)\n",
    "  response = chatbot(chat, max_new_tokens=64)\n",
    "  # print(f\"text: {text}, response: {parse(response[0][\"generated_text\"][-1][\"content\"])}, actual: {map_label(actual)}\")\n",
    "\n",
    "  # validate\n",
    "  if map_label(actual) == parse(response[0][\"generated_text\"][-1][\"content\"]):\n",
    "    success += 1\n",
    "  else:\n",
    "    failure += 1\n",
    "\n",
    "print(f'success rate: {success / total}')\n",
    "print(f'error rate: {failure / total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiQ6A8vif4BX"
   },
   "source": [
    "## Problem 2: Error Analysis\n",
    "\n",
    "Conduct an error analysis on your models. What are your models good at? What do they get wrong? Provide examples of both correct and incorrect predictions. Suggest methods to improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLE0Tn3ef4BX"
   },
   "outputs": [],
   "source": [
    "# See last cell in each section for full error reporting\n",
    "# Encoder: 0.66 accuracy\n",
    "# Decoder: 0.5 accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two examples are from the first BERT encoder model classification, which achieved a passable accuracy of 0.664.\n",
    "\n",
    "text: i was feeling a little vain when i did this one, response: sadness, actual: sadness\n",
    "\n",
    "text: i cant walk into a shop anywhere where i do not feel uncomfortable, response: sadness, actual: fear\n",
    "\n",
    "This model did perform slightly better than the other, but I found that many of the text fragments that we more subtle were often misclassified. The second example is a good indicator of that, as sadness is a fairly reasonable classification (the model was rarely far off base), and it is difficult for even some humans to classify all of these phrases with a relatively subjective emotional category. To improve this model, we could increase the size of the training set to provide the model with further context before generating predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are two examples from the GPT classification, which was much less accurate than the BERT model.\n",
    "\n",
    "text: i was feeling as heartbroken as im sure katniss was, response: sadness, actual: sadness\n",
    "\n",
    "text: i feel a little mellow today, response: sadness, actual: joy\n",
    "\n",
    "The GPT model had a very hard time understanding subtext, and many of the text fragments were a little vague. It often guessed sadness in place of other emotions as well. This particular model is not as advanced as more recent models, so it could be helpful to try other models. In addition to this, more rigorous prompting to increase the level of detail of the specifications could also be useful. Interestingly, in some cases the model would also output something like \"this seems like a combination of two emotions, but I can only specify one,\" indicating its confusion/uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCzBEq_Df4BY"
   },
   "source": [
    "## OPTIONAL. BONUS. Problem 3: Improvements\n",
    "\n",
    "Implement your suggestions for improving the performance. Describe your method and report the results on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DpkZE54Xf4BY"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMCU5PBHz8DF"
   },
   "source": [
    "No other directions for this quiz, other than what's here and in the \"General Directions\" section. You have a lot of freedom with this quiz. Don't get carried away. It is expected the results may vary, being better or worse. Graders are not going to run your notebooks. The notebook will be read as a report on how different models were explored. Since you'll be using libraries, the emphasis will be on your ability to communicate your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VfoAYAQtw4M"
   },
   "source": [
    "## Before You Submit...\n",
    "\n",
    "00. Re-read the general instructions provided above, and\n",
    "01. Hit \"Kernel\"->\"Restart & Run All\". The first cell that is run should show [1], the second should show [2], and so on...\n",
    "02. Submit your notebook (as .ipynb, not PDF) using Gradescope, and\n",
    "03.  Do not submit any other files."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
