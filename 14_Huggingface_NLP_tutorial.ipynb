{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXh4hIsyQf7S"
      },
      "outputs": [],
      "source": [
        "# Transformers installation\n",
        "! pip install transformers datasets evaluate accelerate\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiQDmhZtQf7V"
      },
      "source": [
        "# Quickstart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HfcIFiXQf7W"
      },
      "source": [
        "Transformers is designed to be fast and easy to use so that everyone can start learning or building with transformer models.\n",
        "\n",
        "The number of user-facing abstractions is limited to only three classes for instantiating a model, and two APIs for inference or training. This quickstart introduces you to Transformers' key features and shows you how to:\n",
        "\n",
        "- load a pretrained model\n",
        "- run inference with [Pipeline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.Pipeline)\n",
        "- fine-tune a model with [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iet78ch_Qf7X"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hpsmMdlQf7X"
      },
      "source": [
        "To start, we recommend creating a Hugging Face [account](https://hf.co/join). An account lets you host and access version controlled models, datasets, and [Spaces](https://hf.co/spaces) on the Hugging Face [Hub](https://hf.co/docs/hub/index), a collaborative platform for discovery and building.\n",
        "\n",
        "Create a [User Access Token](https://hf.co/docs/hub/security-tokens#user-access-tokens) and log in to your account.\n",
        "\n",
        "<hfoptions id=\"authenticate\">\n",
        "<hfoption id=\"notebook\">\n",
        "\n",
        "Paste your User Access Token into [notebook_login](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/authentication#huggingface_hub.notebook_login) when prompted to log in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmgTKSDKQf7Y"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRwRJNnIQf7Y"
      },
      "source": [
        "</hfoption>\n",
        "<hfoption id=\"CLI\">\n",
        "\n",
        "Make sure the [huggingface_hub[cli]](https://huggingface.co/docs/huggingface_hub/guides/cli#getting-started) package is installed and run the command below. Paste your User Access Token when prompted to log in.\n",
        "\n",
        "```bash\n",
        "hf auth login\n",
        "```\n",
        "\n",
        "</hfoption>\n",
        "</hfoptions>\n",
        "\n",
        "Install Pytorch.\n",
        "\n",
        "```bash\n",
        "!pip install torch\n",
        "```\n",
        "\n",
        "Then install an up-to-date version of Transformers and some additional libraries from the Hugging Face ecosystem for accessing datasets and vision models, evaluating training, and optimizing training for large models.\n",
        "\n",
        "```bash\n",
        "!pip install -U transformers datasets evaluate accelerate timm\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCmtBOMuQf7Z"
      },
      "source": [
        "## Pretrained models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqHinScYQf7Z"
      },
      "source": [
        "Each pretrained model inherits from three base classes.\n",
        "\n",
        "| **Class** | **Description** |\n",
        "|---|---|\n",
        "| [PreTrainedConfig](https://huggingface.co/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig) | A file that specifies a models attributes such as the number of attention heads or vocabulary size. |\n",
        "| [PreTrainedModel](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel) | A model (or architecture) defined by the model attributes from the configuration file. A pretrained model only returns the raw hidden states. For a specific task, use the appropriate model head to convert the raw hidden states into a meaningful result (for example, [LlamaModel](https://huggingface.co/docs/transformers/main/en/model_doc/llama2#transformers.LlamaModel) versus [LlamaForCausalLM](https://huggingface.co/docs/transformers/main/en/model_doc/llama2#transformers.LlamaForCausalLM)). |\n",
        "| Preprocessor | A class for converting raw inputs (text, images, audio, multimodal) into numerical inputs to the model. For example, [PreTrainedTokenizer](https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer) converts text into tensors and [ImageProcessingMixin](https://huggingface.co/docs/transformers/main/en/main_classes/image_processor#transformers.ImageProcessingMixin) converts pixels into tensors. |\n",
        "\n",
        "We recommend using the [AutoClass](https://huggingface.co/docs/transformers/main/en/./model_doc/auto) API to load models and preprocessors because it automatically infers the appropriate architecture for each task and machine learning framework based on the name or path to the pretrained weights and configuration file.\n",
        "\n",
        "Use [from_pretrained()](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) to load the weights and configuration file from the Hub into the model and preprocessor class.\n",
        "\n",
        "When you load a model, configure the following parameters to ensure the model is optimally loaded.\n",
        "\n",
        "- `device_map=\"auto\"` automatically allocates the model weights to your fastest device first.\n",
        "- `dtype=\"auto\"` directly initializes the model weights in the data type they're stored in, which can help avoid loading the weights twice (PyTorch loads weights in `torch.float32` by default)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OmIByUkQf7a"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-large\", dtype=\"auto\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN0Immo2Qf7a"
      },
      "source": [
        "Tokenize the text and return PyTorch tensors with the tokenizer. Move the model to an accelerator if it's available to accelerate inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljtykjCwQf7a"
      },
      "outputs": [],
      "source": [
        "model_inputs = tokenizer([\"The secret to baking a good cake is \"], return_tensors=\"pt\").to(model.device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs"
      ],
      "metadata": {
        "id": "umtXmfzkiogR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43Aj8i5kQf7b"
      },
      "source": [
        "The model is now ready for inference or training.\n",
        "\n",
        "For inference, pass the tokenized inputs to [generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) to generate text. Decode the token ids back into text with [batch_decode()](https://huggingface.co/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC9LSoWkQf7b"
      },
      "outputs": [],
      "source": [
        "generated_ids = model.generate(**model_inputs, max_length=50)\n",
        "tokenizer.batch_decode(generated_ids)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF7_Yb1fQf7b"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwXARvRPQf7b"
      },
      "source": [
        "The [Pipeline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.Pipeline) class is the most convenient way to inference with a pretrained model. It supports many tasks such as text generation, image segmentation, automatic speech recognition, document question answering, and more.\n",
        "\n",
        "> [!TIP]\n",
        "> Refer to the [Pipeline](https://huggingface.co/docs/transformers/main/en/./main_classes/pipelines) API reference for a complete list of available tasks.\n",
        "\n",
        "Create a [Pipeline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.Pipeline) object and select a task. By default, [Pipeline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.Pipeline) downloads and caches a default pretrained model for a given task. Pass the model name to the `model` parameter to choose a specific model.\n",
        "\n",
        "<hfoptions id=\"pipeline-tasks\">\n",
        "<hfoption id=\"text generation\">\n",
        "\n",
        "Use `Accelerator` to automatically detect an available accelerator for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTuYvsEDQf7b"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from accelerate import Accelerator\n",
        "\n",
        "device = Accelerator().device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Generation"
      ],
      "metadata": {
        "id": "pLeFBHl_lw5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\", model=\"openai-community/gpt2-large\", device=device)"
      ],
      "metadata": {
        "id": "1ZPz69PkluHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIP29afUQf7b"
      },
      "source": [
        "Prompt [Pipeline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.Pipeline) with some initial text to generate more text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqOv7qXMQf7b"
      },
      "outputs": [],
      "source": [
        "generator(\"The secret to baking a good cake is \", max_new_tokens=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question Answering"
      ],
      "metadata": {
        "id": "KcSe79pCl1Wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa = pipeline(\"question-answering\", model=\"distilbert/distilbert-base-cased-distilled-squad\", device=device)"
      ],
      "metadata": {
        "id": "4PV7xc9cl3ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa(question=\"Who wrote Frankenstein?\",\n",
        "    context=\"William Shakespeare wrote Macbeth, Mary Shelley Frankenstein, \\\n",
        "              and Ray Bradbury Fahrenheit\")"
      ],
      "metadata": {
        "id": "WguI9Cm5l9LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarization"
      ],
      "metadata": {
        "id": "HZROuk-vneOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", device=device)"
      ],
      "metadata": {
        "id": "kXTzCVX1nfzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer(\"\"\"\n",
        "The FitnessGramâ„¢ Pacer Test is a multistage aerobic capacity test that\n",
        "progressively gets more difficult as it continues. The 20 meter pacer\n",
        "test will begin in 30 seconds. Line up at the start. The running speed\n",
        "starts slowly, but gets faster each minute after you hear this signal.\n",
        "[beep] A single lap should be completed each time you hear this sound.\n",
        "[ding] Remember to run in a straight line, and run as long as possible.\n",
        "The second time you fail to complete a lap before the sound, your test\n",
        "is over. The test will begin on the word start. On your mark, get ready,\n",
        "start.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "zy2UDx3ZniRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Machine Translation"
      ],
      "metadata": {
        "id": "HLMDyyR0ogPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\", device=device)"
      ],
      "metadata": {
        "id": "9GtK4dZKoh5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator(\"Yesterday, I ate strawberry ice cream and watched the new Chainsaw Man movie\")"
      ],
      "metadata": {
        "id": "73hf2XWuoq7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM Chats"
      ],
      "metadata": {
        "id": "7jF4LsC7qLjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\", device=device)"
      ],
      "metadata": {
        "id": "4sP0XL3wqMrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an evil devil that loves mischief.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hey I found a wallet on the floor, what should I do?\"}\n",
        "]"
      ],
      "metadata": {
        "id": "LtoUhRNUq4mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chatbot(chat, max_new_tokens=64)\n",
        "print(response[0][\"generated_text\"][-1][\"content\"])"
      ],
      "metadata": {
        "id": "4K0bZ3U7rDOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST-tu7v1Qf7d"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-ZLf_4eQf7d"
      },
      "source": [
        "[Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) is a complete training and evaluation loop for PyTorch models. It abstracts away a lot of the boilerplate usually involved in manually writing a training loop, so you can start training faster and focus on training design choices. You only need a model, dataset, a preprocessor, and a data collator to build batches of data from the dataset.\n",
        "\n",
        "Use the [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) class to customize the training process. It provides many options for training, evaluation, and more. Experiment with training hyperparameters and features like batch size, learning rate, mixed precision, torch.compile, and more to meet your training needs. You could also use the default training parameters to quickly produce a baseline.\n",
        "\n",
        "Load a model, tokenizer, and dataset for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ_UJ2FcQf7d"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "dataset = load_dataset(\"ucirvine/sms_spam\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Finagling"
      ],
      "metadata": {
        "id": "kyVNcPoHzd4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datasets come in all shapes and sizes, we have to do some preprocessing befre heading to our model."
      ],
      "metadata": {
        "id": "Is-3p6h-GwzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "HIxa9bf4vkGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset['train'].train_test_split(test_size=0.1)"
      ],
      "metadata": {
        "id": "VNzJGXpGv3dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "RuN8h-fqwyB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = dataset[\"train\"]\n",
        "test = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "jyiNcbL4w3pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "-zkNBCQExQ_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "XgFRNP6ew-Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile Train/Val/Test\n",
        "dataset = DatasetDict({\"train\": train[\"train\"], \"val\": train[\"test\"], \"test\": test})"
      ],
      "metadata": {
        "id": "5JD-zba2xkSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "XSMwl_fQxuzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j97iOXUNQf7d"
      },
      "source": [
        "Create a function to tokenize the text and convert it into PyTorch tensors. Apply this function to the whole dataset with the [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) method."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "h_7VJpD4zkTE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSgAKp5bQf7e"
      },
      "outputs": [],
      "source": [
        "def tokenize_dataset(dataset):\n",
        "    return tokenizer(dataset[\"sms\"])\n",
        "dataset = dataset.map(tokenize_dataset, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "eg5VE7KYvpZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2bCIGl0Qf7e"
      },
      "source": [
        "Load a data collator to create batches of data and pass the tokenizer to it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Collation"
      ],
      "metadata": {
        "id": "Mrtq0EhJzrAM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9m6NbB6oQf7e"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator"
      ],
      "metadata": {
        "id": "xccZ_6-tvsd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85YHEFQgQf7e"
      },
      "source": [
        "Next, set up [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) with the training features and hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Arguments"
      ],
      "metadata": {
        "id": "1pFx2J7OzuDy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq3K_PEKQf7e"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    report_to=\"none\",\n",
        "    output_dir=\"spam-detect\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    logging_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsTgvZ3DQf7e"
      },
      "source": [
        "Finally, pass all these separate components to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) and call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to start."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics"
      ],
      "metadata": {
        "id": "u15T7ody1Obu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "CTqwD-vL1Wry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"accuracy\", \"f1\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "ryyRj2Ym1Q0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainer"
      ],
      "metadata": {
        "id": "gvFOUTsYzwSQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuO2N_ghQf7o"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"val\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics"
      ],
      "metadata": {
        "id": "rkZSUIEPzZQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(dataset[\"test\"])"
      ],
      "metadata": {
        "id": "gC8pJtJ_3Ogm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.metrics"
      ],
      "metadata": {
        "id": "xIbqfW3k5qGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A04Te3GxQf7p"
      },
      "source": [
        "Congratulations, you just trained your first model with Transformers!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Head Tuning"
      ],
      "metadata": {
        "id": "Kj7R_Gtc6Rys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import TrainingArguments\n",
        "from transformers import DataCollatorWithPadding\n",
        "from transformers import Trainer\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "dataset = load_dataset(\"ucirvine/sms_spam\")"
      ],
      "metadata": {
        "id": "NTmQ07Ki6Uqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset['train'].train_test_split(test_size=0.1)"
      ],
      "metadata": {
        "id": "LzAmga7Q6a6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = dataset[\"train\"]\n",
        "test = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "mASHaDIo6iE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "OOyPxpOA6pfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile Train/Val/Test\n",
        "dataset = DatasetDict({\"train\": train[\"train\"], \"val\": train[\"test\"], \"test\": test})"
      ],
      "metadata": {
        "id": "-GUlzUyy6lke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_dataset(dataset):\n",
        "    return tokenizer(dataset[\"sms\"])\n",
        "dataset = dataset.map(tokenize_dataset, batched=True)"
      ],
      "metadata": {
        "id": "AYp862Y66eUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "iNtZ-U7W6sZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look at the components of our models by doing model.parameters() this allows us to look how the model was constructed. We can also \"freeze\" layers by not allowing gradients to flow during back propagation. Super light wieght and efficient to just modify the classification head or create your own!"
      ],
      "metadata": {
        "id": "lPtwi9wJF846"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the parameters of the base model\n",
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# The 'classifier' (the head) is already trainable by default, but just to make sure\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "JnGqU3D96Tw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    report_to=\"none\",\n",
        "    output_dir=\"spam-detect\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    logging_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ],
      "metadata": {
        "id": "H5y5Yc0t6v0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"accuracy\", \"f1\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "IyLuD_nA640q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"val\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "BdRuFPkX67QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "0MhZP8A97Jku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(dataset[\"test\"])"
      ],
      "metadata": {
        "id": "WjVY10RQ7Lya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.metrics"
      ],
      "metadata": {
        "id": "p2NLVYdZ7Nd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PEFT (LoRA)"
      ],
      "metadata": {
        "id": "HXJPMG_f7fZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import TrainingArguments\n",
        "from transformers import DataCollatorWithPadding\n",
        "from transformers import Trainer\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "dataset = load_dataset(\"ucirvine/sms_spam\")"
      ],
      "metadata": {
        "id": "FBeleGu77mQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "R represents the rank of the low-rank approximation.\n",
        "alpha represents the scaling factor for the low-rank approximation.\n",
        "dropout is the dropout probability\n",
        "and target_modules are the layers to apply the low-rank approximation to, we can apply it to our attention or our FFN!\n",
        "\n",
        "We combine the lora model to our base model its basically just adding to matrixes per target layer. Super lightweight and efficient!"
      ],
      "metadata": {
        "id": "2xby5fS3F4c3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=\"all-linear\",\n",
        "    bias=\"none\",\n",
        ")\n",
        "peft_model = get_peft_model(base_model, config)\n",
        "peft_model.print_trainable_parameters() # To verify only a small % is trainable"
      ],
      "metadata": {
        "id": "8ZujBlu87sKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
        "train = dataset[\"train\"]\n",
        "test = dataset[\"test\"]\n",
        "train = train.train_test_split(test_size=0.2)\n",
        "# Compile Train/Val/Test\n",
        "dataset = DatasetDict({\"train\": train[\"train\"], \"val\": train[\"test\"], \"test\": test})"
      ],
      "metadata": {
        "id": "7SgZ1yC-9FnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_dataset(dataset):\n",
        "    return tokenizer(dataset[\"sms\"])\n",
        "dataset = dataset.map(tokenize_dataset, batched=True)"
      ],
      "metadata": {
        "id": "lKqu4by6-F0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "VMUwkqNMCwrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    report_to=\"none\",\n",
        "    output_dir=\"spam-detect\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    logging_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ],
      "metadata": {
        "id": "sHskLxft-VPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"accuracy\", \"f1\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "0cg_ovz0-YIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"val\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "uUVhXx9e-Z8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "3KF-4arM-bYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(dataset[\"test\"])"
      ],
      "metadata": {
        "id": "_WSXl-9N-c0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.metrics"
      ],
      "metadata": {
        "id": "NR5F4M_S-eT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mini Aside: Propmpting Decoders"
      ],
      "metadata": {
        "id": "L10VwwGMDZw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoders can be trained but one major pro for decoders is that they can be tuned without training. We call this prompting! We allow the model to gain context through previous responses to get an answer!"
      ],
      "metadata": {
        "id": "ZOM0CoWnFPEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from accelerate import Accelerator\n",
        "\n",
        "device = Accelerator().device"
      ],
      "metadata": {
        "id": "pczqwCqKDbGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"ucirvine/sms_spam\")"
      ],
      "metadata": {
        "id": "IHLG6lb0DwrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\", device=device)"
      ],
      "metadata": {
        "id": "bKVmFmR4Dl1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "This is a text classification task.\n",
        "sms: {sms}\n",
        "Is this text spam or ham:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Gk21VWfoDy2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"\"\"\n",
        "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\n\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "TffiOEYNHuyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is what we call few shot prompting, we provide a couple of \"training examples\" and let the model prect everything after!\n",
        "\n",
        "Very prone to bias but for smarter models they can learn complex relationships really fast!"
      ],
      "metadata": {
        "id": "uIlTWOV-E60z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a Spam detection bot\"},\n",
        "    {\"role\": \"user\", \"content\": f\"{prompt.format(sms=example)}\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"spam\"}\n",
        "]"
      ],
      "metadata": {
        "id": "rPwgAQCyHW4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat"
      ],
      "metadata": {
        "id": "g2PflOPhHyTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is also parrellizable, Try find out how we can parallelize this function!"
      ],
      "metadata": {
        "id": "em15msvJFHv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_score(dataset, chat):\n",
        "    chat = chat.copy()\n",
        "    chat.append({\"role\": \"user\", \"content\": f\"{prompt.format(sms=dataset['sms'])}\"})\n",
        "    outputs = generator(chat, max_new_tokens=2, pad_token_id=tokenizer.eos_token_id)\n",
        "    dataset[\"guess\"] = outputs[0][\"generated_text\"]\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "IXIkbZY5DsCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_score(dataset[\"train\"][3], chat)"
      ],
      "metadata": {
        "id": "-AGHEUX2Gveq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6fDd8buQf7p"
      },
      "source": [
        "## Next steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTvF-KC3Qf7p"
      },
      "source": [
        "Now that you have a better understanding of Transformers and what it offers, it's time to keep exploring and learning what interests you the most.\n",
        "\n",
        "- **Base classes**: Learn more about the configuration, model and processor classes. This will help you understand how to create and customize models, preprocess different types of inputs (audio, images, multimodal), and how to share your model.\n",
        "- **Inference**: Explore the [Pipeline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.Pipeline) further, inference and chatting with LLMs, agents, and how to optimize inference with your machine learning framework and hardware.\n",
        "- **Training**: Study the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) in more detail, as well as distributed training and optimizing training on specific hardware.\n",
        "- **Quantization**: Reduce memory and storage requirements with quantization and speed up inference by representing weights with fewer bits.\n",
        "- **Resources**: Looking for end-to-end recipes for how to train and inference with a model for a specific task? Check out the task recipes!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}